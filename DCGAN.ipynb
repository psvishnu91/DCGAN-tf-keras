{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the DCGAN paper in tf+keras\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "Please note that the slides here are taken directly from cs231n: http://cs231n.stanford.edu/.\n",
    "I really hope this is not some weird license violation :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "1. Build a generator network in keras. This model will only be used to create fake images and never directly trained.\n",
    "\n",
    "2. Build a discriminator network in keras. This model will be trained using fake labels from the generator and positive labels from the actual data.\n",
    "    * Note: Soumith Chintala (one of the authors of the DCGAN paper) suggests feeding in homogeneous data points in a mini batch while training the discriminator, that is either all fake or all real.\n",
    "    * We might want to also want to keep a pool of fake images from previous minibatches and stochastically feeding into the training. The hope is to prevent `mode collapse` where we always produce the same fake image or oscillate between outputs.\n",
    "\n",
    "3. Make a GAN model which takes the noise input and spits out the discriminator output. That is GAN = D(G(Z)). In this composite model we make the discriminator weights non trainable. The idea is to use the gradients from the loss of the composite model to flow to the input of the discriminator (generator output). Now we can modify the weights of the Generator so that the discriminator would have predicted more true labels (`1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "import keras.layers.core as kcore\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.layers import normalization\n",
    "from keras.layers import convolutional\n",
    "from keras.layers import pooling\n",
    "from keras.layers.core import Flatten\n",
    "from keras_diagram import ascii\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import advanced_activations\n",
    "from keras.regularizers import l1, l2, activity_l2\n",
    "from keras.layers.core import Dropout\n",
    "from IPython.display import display, HTML \n",
    "\n",
    "\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "pd.options.display.max_columns = 40\n",
    "css = open('/nail/home/visp/ipython/style-table.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from os.path import join as path_join\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that we are using `tf` dim ordering which is `(n_rows, n_cols, n_channels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"image_dim_ordering\": \"tf\", \r\n",
      "    \"epsilon\": 1e-07, \r\n",
      "    \"floatx\": \"float32\", \r\n",
      "    \"backend\": \"tensorflow\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "cat ~/.keras/keras.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = '/Users/visp/work/tutorials/tf_tutorials/GAN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "celeb_path = path_join(root_path, 'data/celebA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Config = namedtuple(\n",
    "    'Config',\n",
    "    'num_samples output_size img_path '\n",
    "    'mean_image_path transformed_img_path '\n",
    ")\n",
    "config = Config(\n",
    "    num_samples=10000,\n",
    "    output_size=(64, 64),\n",
    "    img_path=celeb_path,\n",
    "    mean_image_path=path_join(root_path, 'mean_img.npz'),\n",
    "    transformed_img_path=path_join(root_path, 'transf_img.pkl'),\n",
    ")\n",
    "\n",
    "\n",
    "ImgData = namedtuple('ImgData', 'name img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    n_rows = 2\n",
    "    n_cols = 5\n",
    "    for i in xrange(n_rows):\n",
    "        for j in xrange(n_cols):\n",
    "            idx = (i*n_cols) + j + 1\n",
    "            plt.subplot(n_rows, n_cols, idx)\n",
    "            plt.imshow(images[idx].img)\n",
    "            plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and saving the preprocess\n",
    "## (Only do for the first time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls ~/work/tutorials/tf_tutorials/GAN/data/celebA/ | wc -l\n",
    "ls ~/work/tutorials/tf_tutorials/GAN/data/celebA/ | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_input(dir_path, output_size, num_samples=None):\n",
    "    raw_images = []\n",
    "    for i, fl in enumerate(os.listdir(celeb_path)):\n",
    "        if num_samples and i == num_samples:\n",
    "            break\n",
    "        raw_images.append(\n",
    "            ImgData(\n",
    "                name=fl,\n",
    "                img=cv2.resize(\n",
    "                    mpimg.imread(os.path.join(celeb_path, fl)),\n",
    "                    output_size,\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    return raw_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_images = load_input(\n",
    "    dir_path=config.img_path,\n",
    "    num_samples=config.num_samples,\n",
    "    output_size=config.output_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_images(raw_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_images[0].img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_image = np.zeros(\n",
    "    [\n",
    "        config.output_size[0],\n",
    "        config.output_size[1],\n",
    "        3, # num channels\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "print mean_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rim in raw_images:\n",
    "    mean_image += rim.img\n",
    "\n",
    "mean_image /= config.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    file=config.mean_image_path,\n",
    "    mean_image=mean_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image and mean center\n",
    "\n",
    "Mean center the images and map the output to a space of (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_images(img, mean_image):\n",
    "    \"\"\"\n",
    "    \n",
    "    Deletes the mean image. And then scales the image\n",
    "    so that (-255, 255) -> (-1, 1).\n",
    "    \"\"\"\n",
    "    transf_img = img.astype(np.float32)\n",
    "    transf_img -= mean_image\n",
    "    x1, y1, x2, y2 = -255., -1., 255., 1.\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    return (m * (transf_img - x1) + y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_images = load_input(\n",
    "    dir_path=config.img_path,\n",
    "    num_samples=config.num_samples,\n",
    "    output_size=config.output_size,\n",
    ")\n",
    "mean_image = np.load(config.mean_image_path)['mean_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(mean_image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_images(raw_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transf_images = [\n",
    "    ImgData(\n",
    "        name=rim.name,\n",
    "        img=center_images(img=rim.img, mean_image=mean_image),\n",
    "    )\n",
    "    for rim in raw_images\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the min and the max value over all images\n",
    "min_pix, max_pix = np.inf, -np.inf\n",
    "for imd in transf_images:\n",
    "    img_lin = imd.img[:]\n",
    "    img_min, img_max = img_lin.min(), img_lin.max()\n",
    "    if img_min < min_pix:\n",
    "        min_pix = img_min\n",
    "    if img_max > max_pix:\n",
    "        max_pix = img_max\n",
    "\n",
    "print 'min of all transformed pixels:', min_pix\n",
    "print 'max of all transformed pixels:', max_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_images(images=transf_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save transformed image data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open(config.transformed_img_path, 'w') as outfile:\n",
    "#     pickle.dump(\n",
    "#         obj=[\n",
    "#             imd._asdict()\n",
    "#             for imd in transf_images\n",
    "#         ],\n",
    "#         file=outfile,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image(path_join(root_path, 'batch_norm.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Image(path_join(root_path, 'he_activation.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load transformed image data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(config.transformed_img_path) as infile:\n",
    "    transf_image_loaded = pickle.load(infile)\n",
    "    transf_images = [\n",
    "        ImgData(**each_loaded)\n",
    "        for each_loaded in transf_image_loaded\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam_opt = optimizers.Adam(\n",
    "    lr=0.0002,\n",
    "    beta_1=0.5,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    decay=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GENERATOR_Z_DIM = 100\n",
    "GENERATOR_L1_DIM = (4, 4, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KernelSize = namedtuple('KernelSize', 'n_rows n_cols')\n",
    "\n",
    "DEFAULT_KERNEL = KernelSize(\n",
    "    n_rows=5,\n",
    "    n_cols=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activated_convt_layer(\n",
    "        input_layer, n_rows_out, n_cols_out, nb_filter,\n",
    "        name_suffix='', kernel_size=DEFAULT_KERNEL):\n",
    "    convt = convt_layer(\n",
    "        input_layer=input_layer,\n",
    "        n_rows_out=n_rows_out,\n",
    "        n_cols_out=n_cols_out,\n",
    "        nb_filter=nb_filter,\n",
    "        name_suffix=name_suffix,\n",
    "        kernel_size=DEFAULT_KERNEL,\n",
    "        # See the image above on why we use he_normal.\n",
    "        # Since we are using batch norm, this is not very\n",
    "        # important.\n",
    "        kernel_init='he_normal',\n",
    "    )\n",
    "    # Batchnormalization with momentum update for the gradients.\n",
    "    # The batch norm's \\gamma initialized to 1 and \\beta initialized\n",
    "    # to zeros.\n",
    "    # See also https://keras.io/layers/normalization/#batchnormalization\n",
    "    convt_bn = normalization.BatchNormalization(\n",
    "        axis=3,\n",
    "        name='convt_bn_' + name_suffix,\n",
    "    )(convt)\n",
    "    return Activation(\n",
    "        activation='relu',\n",
    "        name='convt_act_' + name_suffix,\n",
    "    )(convt_bn)\n",
    "\n",
    "\n",
    "def convt_layer(\n",
    "        input_layer, n_rows_out, n_cols_out, nb_filter,\n",
    "        name_suffix, kernel_init='he_normal',\n",
    "        kernel_size=DEFAULT_KERNEL):\n",
    "    return convolutional.Deconvolution2D(\n",
    "        nb_filter=nb_filter,\n",
    "        nb_row=kernel_size.n_rows,\n",
    "        nb_col=kernel_size.n_cols,\n",
    "        output_shape=(None, n_rows_out, n_cols_out, nb_filter),\n",
    "        border_mode='same',\n",
    "        subsample=(2, 2),\n",
    "        name='convt_' + name_suffix,\n",
    "        init=kernel_init,\n",
    "    )(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_dim = GENERATOR_L1_DIM\n",
    "\n",
    "z = Input(shape=(GENERATOR_Z_DIM,), dtype='float32', name='Z')\n",
    "z_large = Dense(\n",
    "    output_dim=(l1_dim[0]*l1_dim[1]*l1_dim[2]),\n",
    "    name='G_Z_large',\n",
    ")(z)\n",
    "z_img_reshaped = kcore.Reshape(\n",
    "    target_shape=l1_dim,\n",
    "    name='G_Z_img_reshaped',\n",
    ")(z_large)\n",
    "\n",
    "convt_8_act = activated_convt_layer(\n",
    "    input_layer=z_img_reshaped,\n",
    "    n_rows_out=8,\n",
    "    n_cols_out=8,\n",
    "    nb_filter=512,\n",
    "    name_suffix='G_4_to_8',\n",
    "    kernel_size=KernelSize(\n",
    "        n_rows=2,\n",
    "        n_cols=2,\n",
    "    ),\n",
    ")\n",
    "convt_16_act = activated_convt_layer(\n",
    "    input_layer=convt_8_act,\n",
    "    n_rows_out=16,\n",
    "    n_cols_out=16,\n",
    "    nb_filter=256,\n",
    "    name_suffix='G_8_to_16',\n",
    ")\n",
    "convt_32_act = activated_convt_layer(\n",
    "    input_layer=convt_16_act,\n",
    "    n_rows_out=32,\n",
    "    n_cols_out=32,\n",
    "    nb_filter=128,\n",
    "    name_suffix='G_16_to_32',\n",
    ")\n",
    "# Note that this is not an activated layer\n",
    "# We want the last layer to be a convolution with\n",
    "# a tanh activation and no batch normalization.\n",
    "# Refer to the DCGAN paper.\n",
    "convt_64 = convt_layer(\n",
    "    input_layer=convt_32_act,\n",
    "    n_rows_out=64,\n",
    "    n_cols_out=64,\n",
    "    nb_filter=3,\n",
    "    name_suffix='G_32_to_64',\n",
    "    kernel_init='glorot_normal',\n",
    ")\n",
    "gan_image = Activation(activation='tanh', name='G_gan_image')(convt_64)\n",
    "generator = Model(\n",
    "    input=z,\n",
    "    output=gan_image,\n",
    "    name='generator',\n",
    ")\n",
    "generator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=adam_opt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          InputLayer (None, 100)         \n",
      "               Dense (None, 16384)       \n",
      "             Reshape (None, 4, 4, 1024)  \n",
      "     Deconvolution2D (None, 8, 8, 512)   \n",
      "  BatchNormalization (None, 8, 8, 512)   \n",
      "                Relu (None, 8, 8, 512)   \n",
      "     Deconvolution2D (None, 16, 16, 256) \n",
      "  BatchNormalization (None, 16, 16, 256) \n",
      "                Relu (None, 16, 16, 256) \n",
      "     Deconvolution2D (None, 32, 32, 128) \n",
      "  BatchNormalization (None, 32, 32, 128) \n",
      "                Relu (None, 32, 32, 128) \n",
      "     Deconvolution2D (None, 64, 64, 3)   \n",
      "                Tanh (None, 64, 64, 3)   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ascii(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "Z (InputLayer)                   (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "G_Z_large (Dense)                (None, 16384)         1654784     Z[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "G_Z_img_reshaped (Reshape)       (None, 4, 4, 1024)    0           G_Z_large[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convt_G_4_to_8 (Deconvolution2D) (None, 8, 8, 512)     13107712    G_Z_img_reshaped[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convt_bn_G_4_to_8 (BatchNormaliz (None, 8, 8, 512)     2048        convt_G_4_to_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convt_act_G_4_to_8 (Activation)  (None, 8, 8, 512)     0           convt_bn_G_4_to_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convt_G_8_to_16 (Deconvolution2D (None, 16, 16, 256)   3277056     convt_act_G_4_to_8[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convt_bn_G_8_to_16 (BatchNormali (None, 16, 16, 256)   1024        convt_G_8_to_16[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convt_act_G_8_to_16 (Activation) (None, 16, 16, 256)   0           convt_bn_G_8_to_16[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convt_G_16_to_32 (Deconvolution2 (None, 32, 32, 128)   819328      convt_act_G_8_to_16[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "convt_bn_G_16_to_32 (BatchNormal (None, 32, 32, 128)   512         convt_G_16_to_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convt_act_G_16_to_32 (Activation (None, 32, 32, 128)   0           convt_bn_G_16_to_32[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "convt_G_32_to_64 (Deconvolution2 (None, 64, 64, 3)     9603        convt_act_G_16_to_32[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "G_gan_image (Activation)         (None, 64, 64, 3)     0           convt_G_32_to_64[0][0]           \n",
      "====================================================================================================\n",
      "Total params: 18,872,067\n",
      "Trainable params: 18,870,275\n",
      "Non-trainable params: 1,792\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def activated_conv_layer(\n",
    "        input_layer, n_rows_out, n_cols_out, nb_filter,\n",
    "        name_suffix='', kernel_size=DEFAULT_KERNEL):\n",
    "    conv = conv_layer(\n",
    "        input_layer=input_layer,\n",
    "        n_rows_out=n_rows_out,\n",
    "        n_cols_out=n_cols_out,\n",
    "        nb_filter=nb_filter,\n",
    "        name_suffix=name_suffix,\n",
    "        kernel_size=DEFAULT_KERNEL,\n",
    "        # See the image above on why we use he_normal.\n",
    "        # Since we are using batch norm, this is not very\n",
    "        # important.\n",
    "        kernel_init='he_normal',\n",
    "    )\n",
    "    # Batchnormalization with momentum update for the gradients.\n",
    "    # The batch norm's \\gamma initialized to 1 and \\beta initialized\n",
    "    # to zeros.\n",
    "    # See also https://keras.io/layers/normalization/#batchnormalization\n",
    "    # We have to use mode=2 for model sharing. See\n",
    "    # https://github.com/fchollet/keras/issues/2827 why.\n",
    "    conv_bn = normalization.BatchNormalization(\n",
    "        axis=3,\n",
    "        mode=2,\n",
    "        name='conv_bn_' + name_suffix,\n",
    "    )(conv)\n",
    "    return advanced_activations.LeakyReLU(alpha=0.2)(conv_bn)\n",
    "\n",
    "\n",
    "def conv_layer(\n",
    "        input_layer, n_rows_out, n_cols_out, nb_filter,\n",
    "        name_suffix, kernel_init='he_normal',\n",
    "        kernel_size=DEFAULT_KERNEL):\n",
    "    return convolutional.Convolution2D(\n",
    "        nb_filter=nb_filter,\n",
    "        nb_row=kernel_size.n_rows,\n",
    "        nb_col=kernel_size.n_cols,\n",
    "        border_mode='same',\n",
    "        subsample=(2, 2),\n",
    "        name='conv_' + name_suffix,\n",
    "        init=kernel_init,\n",
    "    )(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          InputLayer (None, 64, 64, 3)   \n",
      "       Convolution2D (None, 32, 32, 128) \n",
      "  BatchNormalization (None, 32, 32, 128) \n",
      "           LeakyReLU (None, 32, 32, 128) \n",
      "       Convolution2D (None, 16, 16, 256) \n",
      "  BatchNormalization (None, 16, 16, 256) \n",
      "           LeakyReLU (None, 16, 16, 256) \n",
      "       Convolution2D (None, 8, 8, 512)   \n",
      "  BatchNormalization (None, 8, 8, 512)   \n",
      "           LeakyReLU (None, 8, 8, 512)   \n",
      "             Flatten (None, 32768)       \n",
      "               Dense (None, 1)           \n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "D_input_img (InputLayer)         (None, 64, 64, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_D_64_to_32 (Convolution2D)  (None, 32, 32, 128)   9728        D_input_img[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv_bn_D_64_to_32 (BatchNormali (None, 32, 32, 128)   512         conv_D_64_to_32[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_40 (LeakyReLU)         (None, 32, 32, 128)   0           conv_bn_D_64_to_32[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_D_32_to_16 (Convolution2D)  (None, 16, 16, 256)   819456      leakyrelu_40[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv_bn_D_32_to_16 (BatchNormali (None, 16, 16, 256)   1024        conv_D_32_to_16[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_41 (LeakyReLU)         (None, 16, 16, 256)   0           conv_bn_D_32_to_16[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv_D_16_to_8 (Convolution2D)   (None, 8, 8, 512)     3277312     leakyrelu_41[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv_bn_D_16_to_8 (BatchNormaliz (None, 8, 8, 512)     2048        conv_D_16_to_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_42 (LeakyReLU)         (None, 8, 8, 512)     0           conv_bn_D_16_to_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "D_last_conv_flat (Flatten)       (None, 32768)         0           leakyrelu_42[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "D_pred (Dense)                   (None, 1)             32769       D_last_conv_flat[0][0]           \n",
      "====================================================================================================\n",
      "Total params: 4,142,849\n",
      "Trainable params: 4,141,057\n",
      "Non-trainable params: 1,792\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_input_img = Input(shape=(64, 64, 3), dtype='float32', name='D_input_img')\n",
    "\n",
    "conv_32_act = activated_conv_layer(\n",
    "    input_layer=d_input_img,\n",
    "    n_rows_out=32,\n",
    "    n_cols_out=32,\n",
    "    nb_filter=128,\n",
    "    name_suffix='D_64_to_32',\n",
    ")\n",
    "conv_16_act = activated_conv_layer(\n",
    "    input_layer=conv_32_act,\n",
    "    n_rows_out=16,\n",
    "    n_cols_out=16,\n",
    "    nb_filter=256,\n",
    "    name_suffix='D_32_to_16',\n",
    ")\n",
    "conv_8_act = activated_conv_layer(\n",
    "    input_layer=conv_16_act,\n",
    "    n_rows_out=8,\n",
    "    n_cols_out=8,\n",
    "    nb_filter=512,\n",
    "    name_suffix='D_16_to_8',\n",
    ")\n",
    "last_conv_flat = Flatten(name='D_last_conv_flat')(conv_8_act)\n",
    "d_pred = Dense(1, activation='sigmoid', name='D_pred')(last_conv_flat)\n",
    "discriminator = Model(input=d_input_img, output=d_pred)\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=adam_opt,\n",
    "    name='discriminator',\n",
    ")\n",
    "print ascii(discriminator)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GAN: Composite model to train G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_trainable(model, is_trainable):\n",
    "    \"\"\"Makes a model (not) trainable.\n",
    "  \n",
    "    .. note::\n",
    "  \n",
    "        This mutates the model in place.\n",
    "    \"\"\"\n",
    "    model.trainable = is_trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = is_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          InputLayer (None, 100)         \n",
      "               Dense (None, 16384)       \n",
      "             Reshape (None, 4, 4, 1024)  \n",
      "     Deconvolution2D (None, 8, 8, 512)   \n",
      "  BatchNormalization (None, 8, 8, 512)   \n",
      "                Relu (None, 8, 8, 512)   \n",
      "     Deconvolution2D (None, 16, 16, 256) \n",
      "  BatchNormalization (None, 16, 16, 256) \n",
      "                Relu (None, 16, 16, 256) \n",
      "     Deconvolution2D (None, 32, 32, 128) \n",
      "  BatchNormalization (None, 32, 32, 128) \n",
      "                Relu (None, 32, 32, 128) \n",
      "     Deconvolution2D (None, 64, 64, 3)   \n",
      "                Tanh (None, 64, 64, 3)   \n",
      "               Model (None, 1)           \n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "gan_Z (InputLayer)               (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "generator (Model)                (None, 64, 64, 3)     18872067    gan_Z[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "model_26 (Model)                 (None, 1)             4142849     generator[5][0]                  \n",
      "====================================================================================================\n",
      "Total params: 23,014,916\n",
      "Trainable params: 18,870,275\n",
      "Non-trainable params: 4,144,641\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "make_trainable(model=discriminator, is_trainable=False)\n",
    "gan_input_z = Input(shape=(GENERATOR_Z_DIM,), dtype='float32', name='gan_Z')\n",
    "generator_img_out = generator(gan_input_z)\n",
    "# We want the discriminator to predict all true for the fake\n",
    "# labels here. Remember this networks sole purpose is to train\n",
    "# the Generator.\n",
    "disc_pred_for_fake = discriminator(generator_img_out)\n",
    "gan_composite = Model(input=gan_input_z, output=disc_pred_for_fake)\n",
    "gan_composite.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=adam_opt,\n",
    "    name='gan_composite',\n",
    ")\n",
    "print ascii(gan_composite)\n",
    "gan_composite.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras.callbacks.TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions\n",
    "\n",
    "- Function to create noise input for the generator.\n",
    "- Function to create fake_images from the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_z(n_samples, z_dim=100):\n",
    "    return np.random.uniform(0, 1, size=[n_samples, z_dim])\n",
    "\n",
    "    \n",
    "def generate_fake_images(generator, n_samples, z_dim=100):\n",
    "    return generator.predict(\n",
    "        generate_z(n_samples, z_dim),\n",
    "        batch_size=250,\n",
    "        verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOMOGENEOUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_disc_minibatch(generator, real_images, batch_size, n_batches=2):\n",
    "    \"\"\"Produces discriminator training data.\n",
    "    \n",
    "    :param real_images:\n",
    "    :type real_images: ImgData\n",
    "    \n",
    "    n_batches must be even because we pass\n",
    "    in only homogeneous data in each mini batch. That\n",
    "    is either all fake or all real. Sice we want equal\n",
    "    num of each, n_batches needs to be even.\n",
    "    \"\"\"\n",
    "    if n_batches % 2 != 0:\n",
    "        raise ValueError('n_batches must be even but is %d', n_batches)\n",
    "    \n",
    "    real_images = [rid.img for rid in real_images]\n",
    "    random.shuffle(real_images)\n",
    "    for i in xrange(n_batches):\n",
    "        if i % 2 == 0:\n",
    "            x_batch = generate_fake_images(\n",
    "                generator=generator,\n",
    "                n_samples=batch_size,\n",
    "            )\n",
    "            yield (np.array(x_batch), [0.]*len(x_batch))\n",
    "        else:\n",
    "            j = i//2\n",
    "            x_batch = real_images[j*batch_size:(j+1)*batch_size]\n",
    "            yield (np.array(x_batch), [1.]*len(x_batch))\n",
    "\n",
    "def iter_disc_minibatch_mix(generator, real_images, batch_size, n_batches=2):\n",
    "    \"\"\"Produces discriminator training data.\n",
    "    \n",
    "    :param real_images:\n",
    "    :type real_images: ImgData\n",
    "    \"\"\"\n",
    "    real_images = [rid.img for rid in real_images]\n",
    "    random.shuffle(real_images)\n",
    "    half_batch = batch_size//2\n",
    "    rem_batch = batch_size-half_batch\n",
    "    for i in xrange(n_batches):\n",
    "        x_fake = generate_fake_images(\n",
    "            generator=generator,\n",
    "            n_samples=half_batch,\n",
    "        )\n",
    "        x_real = real_images[i*rem_batch:(i+1)*rem_batch]\n",
    "        x_batch = x_fake+x_real\n",
    "        y_batch = [0.]*half_batch + [1.]*rem_batch\n",
    "        batch_data = zip(x_batch, y_batch)\n",
    "        random.shuffle(batch_data)\n",
    "        x_batch_shuf, y_batch_shuf = zip(*batch_data)\n",
    "        yield (np.array(x_batch_shuf), list(y_batch_shuf))\n",
    "        \n",
    "def disc_all_data(generator, real_images, n_samples=None):\n",
    "    if n_samples:\n",
    "        real_images = real_images[:n_samples//2]\n",
    "    \n",
    "    real_images = [rid.img for rid in real_images]\n",
    "    n_samples = len(real_images)\n",
    "    x_fake = generate_fake_images(\n",
    "        generator=generator,\n",
    "        n_samples=n_samples,\n",
    "    ).tolist()\n",
    "    x = x_fake+real_images\n",
    "    y = [0.]*n_samples + [1.]*n_samples\n",
    "    data = zip(x, y)\n",
    "    random.shuffle(data)\n",
    "    x_shuf, y_shuf = zip(*data)\n",
    "    return (np.array(x_shuf), list(y_shuf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TrainConfig = namedtuple(\n",
    "    'TrainConfig',\n",
    "    (\n",
    "        'batch_size '\n",
    "        'd_pretrain_n_batches '\n",
    "        'd_pretrain_n_epoch'\n",
    "    ),\n",
    ")\n",
    "train_config = TrainConfig(\n",
    "    batch_size=126,\n",
    "    d_pretrain_n_batches=10,\n",
    "    d_pretrain_n_epoch=1,\n",
    ")\n",
    "\n",
    "d_initial_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_trainable(discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if HOMOGENEOUS:\n",
    "    for i in xrange(train_config.d_pretrain_n_epoch):\n",
    "        for j, batch_data in enumerate(\n",
    "            iter_disc_minibatch_mix(\n",
    "                generator=generator,\n",
    "                real_images=transf_images,\n",
    "                batch_size=train_config.batch_size,\n",
    "                n_batches=train_config.d_pretrain_n_batches,\n",
    "            )\n",
    "        ):\n",
    "            if j %10==0:\n",
    "                print j, '/', train_config.d_pretrain_n_batches\n",
    "            discriminator.fit(\n",
    "                x=batch_data[0],\n",
    "                y=batch_data[1],\n",
    "                batch_size=train_config.batch_size,\n",
    "                nb_epoch=1,\n",
    "                verbose=0,\n",
    "                callbacks=None,\n",
    "                validation_split=0.2,\n",
    "                initial_epoch=d_initial_epoch\n",
    "            )\n",
    "            d_initial_epoch += 1\n",
    "else:\n",
    "    d_train_data = disc_all_data(\n",
    "        generator=generator,\n",
    "        real_images=transf_images,\n",
    "        n_samples=5000,\n",
    "    )\n",
    "    print 'generated data'\n",
    "    discriminator.fit(\n",
    "        x=d_train_data[0],\n",
    "        y=d_train_data[1],\n",
    "        batch_size=train_config.batch_size,\n",
    "        nb_epoch=5,\n",
    "        verbose=1,\n",
    "        callbacks=None,\n",
    "        validation_split=0.2,\n",
    "        initial_epoch=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the pretrained disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 14s    \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X_d_val, Y_d_val = disc_all_data(generator, transf_images, n_samples=1000)\n",
    "pred_d_val = discriminator.predict(X_d_val).ravel()\n",
    "print skmetrics.accuracy_score(y_true=Y_d_val, y_pred=(pred_d_val>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.000000e+03\n",
       "mean     5.000914e-01\n",
       "std      5.001158e-01\n",
       "min      8.405703e-07\n",
       "25%      1.914216e-05\n",
       "50%      4.961875e-01\n",
       "75%      1.000000e+00\n",
       "max      1.000000e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred_d_val).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
